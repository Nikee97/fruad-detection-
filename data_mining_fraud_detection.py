# -*- coding: utf-8 -*-
"""Data Mining Fraud Detection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DEu1BC8gohEgt4-o9q7m8bPiyAF1WnNx

# Project on Fraud Detection 
## Submitted by: 
## Anoushka Motwani (20)
## Nikita Patel (24)

### Data Source: Kaggle 
### Data Type: Textual and Numerical 
### Data Size: 182MB
### Data Domain: Finance/Banking Sector
"""

from google.colab import files
files.upload()

!ls
!pip install -q kaggle
!pip install -q sklearn
!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json
!kaggle datasets download -d ntnu-testimon/paysim1

from google.colab import drive
drive.mount('/content/drive')

!unzip -uq "/content/paysim1.zip" -d "/content/"

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
# %matplotlib inline
import matplotlib.pyplot as plt
import matplotlib.lines as mlines
import seaborn as sns
from sklearn.model_selection import train_test_split, learning_curve
from sklearn.metrics import average_precision_score
import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)

df = pd.read_csv("/content/PS_20174392719_1491204439457_log.csv")
df = df.rename(columns={'oldbalanceOrg':'oldBalanceOrig', 'newbalanceOrig':'newBalanceOrig', \
                        'oldbalanceDest':'oldBalanceDest', 'newbalanceDest':'newBalanceDest'})
df.head()

"""### Data Description: 

1. **step** -Maps a unit of time in the real world. In this case 1 step is 1 hour of time.
2. **type** -CASH-IN, CASH-OUT, DEBIT, PAYMENT and TRANSFER
3. **amount** -amount of the transaction in local currency
4.**nameOrigcustomer** -who started the transaction
5. **oldbalanceOrg** -initial balance before the transaction
6. **newbalanceOrig** -customer's balance after the transaction.
7. **nameDest** -recipient ID of the transaction.
8. **oldbalanceDest** -initial recipient balance before the transaction.
9. **newbalanceDest** -recipient's balance after the transaction.
10. **isFraud** -identifies a fraudulent transaction (1) and non fraudulent (0)
11. **isFlaggedFraud** -flags illegal attempts to transfer more than 200.000 in a single transaction.
"""

df.info()

df.shape

df.size

"""There are a few transactions that have very large amounts. Also, the mean of isFraud is 0.00129, meaning there are ~1.2 frauds per 1000 transactions."""

df.describe()

"""# Exploratory Data Analysis (EDA)"""

df.isna().values.any()

print(df.type.value_counts())

f, ax = plt.subplots(1, 1, figsize=(8, 8))
df.type.value_counts().plot(kind='bar', title="Transaction type", ax=ax, figsize=(8,8))
plt.show()

"""Here we can see that fraudulent transactions are only in 2 types of data. 



1.   Cash_out &
2.   Transfer
"""

ax = df.groupby(['type', 'isFraud']).size().plot(kind='bar')
ax.set_title("Number of transactions which are the actual fraud per transaction type")
ax.set_xlabel("(Type, isFraud)")
ax.set_ylabel("Count of transaction")
for p in ax.patches:
    ax.annotate(str(format(int(p.get_height()), ',d')), (p.get_x(), p.get_height()*1.01))

"""We see that the system has flagged only 16 'Transfer' transactions as Fraud"""

ax = df.groupby(['type', 'isFlaggedFraud']).size().plot(kind='bar')
ax.set_title("Number of transaction which is flagged as fraud per transaction type")
ax.set_xlabel("(Type, isFlaggedFraud)")
ax.set_ylabel("Count of transaction")
for p in ax.patches:
    ax.annotate(str(format(int(p.get_height()), ',d')), (p.get_x(), p.get_height()*1.01))

"""We look at those records and compare with the records which the system cannot catch.

The plot below will focus only on transfer transaction type.

The isFlaggedFraud variable relies on oldbalanceDest, which is 0 and some threshold on the amount variable.
"""

fig, axs = plt.subplots(2, 2, figsize=(10, 10))
tmp = df.loc[(df.type == 'TRANSFER'), :]

a = sns.boxplot(x = 'isFlaggedFraud', y = 'amount', data = tmp, ax=axs[0][0])
axs[0][0].set_yscale('log')
b = sns.boxplot(x = 'isFlaggedFraud', y = 'oldBalanceDest', data = tmp, ax=axs[0][1])
axs[0][1].set(ylim=(0, 0.5e8))
c = sns.boxplot(x = 'isFlaggedFraud', y = 'oldBalanceOrig', data=tmp, ax=axs[1][0])
axs[1][0].set(ylim=(0, 3e7))
d = sns.regplot(x = 'oldBalanceOrig', y = 'amount', data=tmp.loc[(tmp.isFlaggedFraud ==1), :], ax=axs[1][1])
plt.show()

"""Co-Relation plot for all nummeric data"""

df_corr = df[['amount', 'oldBalanceOrig', 'oldBalanceDest','newBalanceOrig','newBalanceDest', 'isFraud']]

fig = plt.subplots(figsize= (10,10), dpi =80)
sns.set(font_scale =1.5)
sns.heatmap(df_corr.corr(), square=True, cbar=True, annot=True, annot_kws={'size': 10})
plt.show()

"""By looking at boxplots for amount, we see that fraudulent activities tend to have larger amounts."""

plt.figure(figsize=(12,8))
sns.boxplot(x = 'isFraud', y = 'amount', data = df[df.amount < 1e5])

plt.figure(figsize=(12,8))
sns.boxplot(hue = 'isFraud', x = 'type', y = 'amount', data = df[df.amount < 1e5])

"""# Data Modeling (Cleaning and Feature Engineering)"""

dataframe = df.loc[(df.type == 'TRANSFER') | (df.type == 'CASH_OUT')]

randomState = 5
np.random.seed(randomState)

#X = X.loc[np.random.choice(X.index, 100000, replace = False)]

Y = dataframe['isFraud']
del dataframe['isFraud']

# Eliminate columns shown to be irrelevant for analysis in the EDA
dataframe = dataframe.drop(['nameOrig', 'nameDest', 'isFlaggedFraud'], axis = 1)

# Binary-encoding of labelled data in 'type'
dataframe.loc[dataframe.type == 'TRANSFER', 'type'] = 0
dataframe.loc[dataframe.type == 'CASH_OUT', 'type'] = 1
dataframe.type = dataframe.type.astype(int) # convert dtype('O') to dtype(int)

dataframe.head()

dataframefraud = dataframe.loc[Y == 1]
dataframenonFraud = dataframe.loc[Y == 0]
print('\nThe fraction of fraudulent transactions with \'oldBalanceDest\' = \
\'newBalanceDest\' = 0 although the transacted \'amount\' is non-zero is: {}'.\
format(len(dataframefraud.loc[(dataframefraud.oldBalanceDest == 0) & \
(dataframefraud.newBalanceDest == 0) & (dataframefraud.amount)]) / (1.0 * len(dataframefraud))))

print('\nThe fraction of genuine transactions with \'oldBalanceDest\' = \
newBalanceDest\' = 0 although the transacted \'amount\' is non-zero is: {}'.\
format(len(dataframenonFraud.loc[(dataframenonFraud.oldBalanceDest == 0) & \
(dataframenonFraud.newBalanceDest == 0) & (dataframenonFraud.amount)]) / (1.0 * len(dataframenonFraud))))

dataframe.loc[(dataframe.oldBalanceDest == 0) & (dataframe.newBalanceDest == 0) & (dataframe.amount != 0), \
      ['oldBalanceDest', 'newBalanceDest']] = - 1

import matplotlib.cm as cm
def correlation_plot(df):
    fig = plt.figure(figsize=(10, 10))
    ax1 = fig.add_subplot(111)
    cmap = cm.get_cmap('jet', 30)
    cax = ax1.imshow(df.corr(), interpolation = "nearest", cmap = cmap)
    ax1.grid(True)
    plt.title("Correlation Heatmap")
    labels = df.columns.tolist()
    ax1.set_xticklabels(labels, fontsize=13, rotation=45)
    ax1.set_yticklabels(labels, fontsize=13)
    fig.colorbar(cax)
    plt.show()
    
correlation_plot(dataframe)

# Alternatively, we can use quick seaborn
# plot the heatmap
#sns.heatmap(X.corr())

dataframe['errorBalanceOrig'] = dataframe.newBalanceOrig + dataframe.amount - dataframe.oldBalanceOrig
dataframe['errorBalanceDest'] = dataframe.oldBalanceDest + dataframe.amount - dataframe.newBalanceDest

dataframe.head()

"""# Model Fitting"""

from sklearn.model_selection import train_test_split, learning_curve
from sklearn.metrics import average_precision_score
from xgboost.sklearn import XGBClassifier
from xgboost import plot_importance, to_graphviz

print('skew = {}'.format( len(dataframefraud) / float(len(dataframe)) ))

"""Splitting data into train and test sets in a 80:20 ratio"""

trainX, testX, trainY, testY = train_test_split(dataframe, Y, test_size = 0.2, \
                                                random_state = randomState)

# Long computation in this cell (~1.8 minutes)
weights = (Y == 0).sum() / (1.0 * (Y == 1).sum())
clf = XGBClassifier(max_depth = 3, scale_pos_weight = weights, \
                n_jobs = 4)
probabilities = clf.fit(trainX, trainY).predict_proba(testX)
print('AUPRC = {}'.format(average_precision_score(testY, \
                                              probabilities[:, 1])))

fig = plt.figure(figsize = (14, 9))
ax = fig.add_subplot(111)

colours = plt.cm.Set1(np.linspace(0, 1, 9))

ax = plot_importance(clf, height = 1, color = colours, grid = False, \
                     show_values = False, importance_type = 'cover', ax = ax);
for axis in ['top','bottom','left','right']:
            ax.spines[axis].set_linewidth(2)
        
ax.set_xlabel('importance score', size = 16);
ax.set_ylabel('features', size = 16);
ax.set_yticklabels(ax.get_yticklabels(), size = 12);
ax.set_title('Ordering of features by importance to the model learnt', size = 20);

# Long computation in this cell (~6 minutes)

trainSizes, trainScores, crossValScores = learning_curve(\
XGBClassifier(max_depth = 3, scale_pos_weight = weights, n_jobs = 4), trainX,\
                                         trainY, scoring = 'average_precision')

trainScoresMean = np.mean(trainScores, axis=1)
trainScoresStd = np.std(trainScores, axis=1)
crossValScoresMean = np.mean(crossValScores, axis=1)
crossValScoresStd = np.std(crossValScores, axis=1)

colours = plt.cm.tab10(np.linspace(0, 1, 9))

fig = plt.figure(figsize = (14, 9))
plt.fill_between(trainSizes, trainScoresMean - trainScoresStd,
    trainScoresMean + trainScoresStd, alpha=0.1, color=colours[0])
plt.fill_between(trainSizes, crossValScoresMean - crossValScoresStd,
    crossValScoresMean + crossValScoresStd, alpha=0.1, color=colours[1])
plt.plot(trainSizes, trainScores.mean(axis = 1), 'o-', label = 'train', \
         color = colours[0])
plt.plot(trainSizes, crossValScores.mean(axis = 1), 'o-', label = 'cross-val', \
         color = colours[1])

ax = plt.gca()
for axis in ['top','bottom','left','right']:
    ax.spines[axis].set_linewidth(2)

handles, labels = ax.get_legend_handles_labels()
plt.legend(handles, ['train', 'cross-val'], bbox_to_anchor=(0.8, 0.15), \
               loc=2, borderaxespad=0, fontsize = 16);
plt.xlabel('training set size', size = 16); 
plt.ylabel('AUPRC', size = 16)
plt.title('Learning curves indicate slightly underfit model', size = 20);

